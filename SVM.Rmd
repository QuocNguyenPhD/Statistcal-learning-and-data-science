---
title: "Data splitting"
author: "Sarah McReynolds"
date: '2022-04-13'
output:
  word_document: default
  pdf_document: default
---


Original data set
```{r}

marketing_campaign=marketing_campaign.csv
dim(marketing_campaign)
```

```{r}
set.seed(1)

#remove NA values
na.omit(marketing_campaign)

#remove unwanted predictors


marketing_campaign$ID <- NULL
marketing_campaign <- marketing_campaign[, -c(2:3)]
marketing_campaign$Dt_Customer <- NULL
marketing_campaign <- marketing_campaign[, -c(17:19)]
marketing_campaign <- marketing_campaign[, -c(20:21)]

#make appropiate columns as.factor
factcols <- c("AcceptedCmp1", "AcceptedCmp2", "Complain", "Response")
numcols <- c("Income", "Teenhome")
marketing_campaign[factcols] <- lapply(marketing_campaign[factcols], as.factor)
marketing_campaign[numcols] <- lapply(marketing_campaign[numcols], as.numeric)


```

```{r}

#scale the data, only the numeric data types can be scaled
ind <- sapply(marketing_campaign, is.numeric)
marketing_campaign[ind] <- lapply(marketing_campaign[ind], scale)

set.seed(1)

#splitting into train and test
n <- nrow(marketing_campaign)
train <- sample(1:n, 0.8*n)
x.train <- marketing_campaign[train,]
x.test <- marketing_campaign[-train,]
#y.train <- marketing_campaign$Response[train]
#y.test <- marketing_campaign$Response[-train]

```


# Support Vector Machine

## Goal:

Our goal is to find the best models to separate the different classes in our data set. Since our response variable only has two level, we will use the SVM one vs. one classifier.

We will determine the best kernel, and the best model for this data set.

It is very unlikely that our data will have a linear boundary. Thus, we will train our model with different kernels to find the one which suit our purpose better. Via cross validation, we will determine the best parameters associated with each kernel.


# Cross Valiation

## Linear Kernel
we will use the following list of cost value: cost = c(1, 2, 5, 10, 100, 0.1, 0.01, 0.05, 0.001, 3, 0.15)

```{r}
library(e1071)

tune.out.lin <- tune(svm, Response~.,  data = x.train, kernel = 'linear', ranges = list(cost = c(0.001,0.01,0.05,0.1,0.15, 1, 2, 3, 5, 10 )))
```


### Summary of the Cross Validation performances
```{r}
summary(tune.out.lin)
```

Running a 10 fold cross validation, our best __cost is 1__ and the __cross validation error rate is 13.88%.__




## Polynomial Kernel

we will use the following degree values: c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10).

```{r}
tune.out.poly <- tune(svm, Response~.,  data = x.train, kernel = 'polynomial', ranges = list(cost = c(0.001,0.01,0.05,0.1,0.15, 1, 2, 3, 5, 10 ), degree = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)))
```

### Summary of the Cross Validation performances
```{r}
summary(tune.out.poly)
```
The __cross validation error obtained is 13.26%.___ This performance is a little better than the one of the linear model.

The best parameters are __cost = 2__ and __degree = 2__.



## Radial Kernel

we will use  gamma = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 0.5)
```{r}
tune.out.rad <- tune(svm, Response~.,  data = x.train, kernel = 'radial', ranges = list(cost = c(0.001,0.01,0.05,0.1,0.15, 1, 2, 3, 5, 10 ), gamma = c(1, 0.5, 0.01, 2, 3, 5, 10, 8, 9, 7)))
```

### Summary of the Cross Validation performances

```{r}
summary(tune.out.rad)
```


The cross validation training error is 12.64% which beats the performance of the both the linear and polynomial model.


## Comparison

```{r}
cat("Linear Kernel Cross-Val error: ", tune.out.lin$best.performance)

cat("\nPolynomial Kernel Cross-Val error: ", tune.out.poly$best.performance)

cat("\nRadial Kernel Cross-Val error: ", tune.out.rad$best.performance)
```

The radial model gives us the best cross validation error. Let us use the best model from each the corss validation performance above to compute the training error rate. Then we will choose the best model to make predictions.

## Trainnig Error Rate


1. I think we should scale the data set before splitting it
2. For the svm, we cannot separate the predictors from the response, Thus, I kept it with the original data set. we must choose how we want it to be on the last project.


```{r}
library(stats)
training.error.lin=mean(predict(tune.out.lin$best.model, x.train[,-20]) != x.train[c(1:1772),20])
training.error.lin
```


```{r}
training.error.poly=mean(predict(tune.out.poly$best.model, x.train[-20]) != x.train[c(1:1772),20])
training.error.poly
```

```{r}
training.error.rad=mean(predict(tune.out.rad$best.model, x.train) != x.train[c(1:1772),20])
training.error.rad
```
the radial kernel give us the best training model with the error rate is about  0.1774554

## testing error
```{r}
testing.error.lin=mean(predict(tune.out.lin$best.model, x.test[,-20]) != x.test[c(1:444),20])
testing.error.lin
```

```{r}
testing.error.poly =mean(predict(tune.out.poly$best.model, x.test) != x.test[c(1:444),20])
testing.error.poly
```

```{r}
testing.error.rad =mean(predict(tune.out.rad$best.model, x.test) != x.test[c(1:444),20])
testing.error.rad
```
the radial give us the best testing error

## cross validation error at different cost
### polynomial
```{r}
poly.cost =data.frame(tune.out.poly$performance)
poly.cost= data.frame(poly.cost[c(11:20),])
```
### radial
```{r}
rad.cost =data.frame(tune.out.rad$performance)
rad.cost= data.frame(rad.cost[c(21:30),])
```

```{r}
plot(tune.out.lin$performances$cost,tune.out.lin$performance$error, type = 'b', col="red",lty=1, ylim = c(0.12, 0.146), main = "10-Fold Cross Validation",xlab= "Cost", ylab="Error")
lines(poly.cost$cost,poly.cost$error, pch=2,type="b" , col= "blue", lty=2)
lines(rad.cost$cost,rad.cost$error, pch=4 ,type= "b",col="green", lty=3)
legend( 0,0.128 ,legend=c('linear', 'polynomial','radial '), col = c("red", "blue", "green"), lty=1:3 )
```
```{r}

plot(svm(x.train[c(1:1772),20]~.,
         x.train[c(1:1772),-20],
         kernel="linear",
         cost=0.1),
     x.train[c(1:1772),])
```

